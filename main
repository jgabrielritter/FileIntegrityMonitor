#!/usr/bin/env python3
"""
File Integrity Monitor

This script monitors files and directories for unauthorized changes by comparing
hash values of files against a baseline.
"""

import os
import sys
import json
import time
import hashlib
import argparse
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("fim.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FileIntegrityMonitor:
    """
    A class to monitor file integrity using hash comparisons.
    """
    
    def __init__(self, config_file: str = "fim_config.json"):
        """
        Initialize the File Integrity Monitor.
        
        Args:
            config_file: Path to the configuration file.
        """
        self.config_file = config_file
        self.config = self._load_config()
        self.baseline_file = self.config.get("baseline_file", "baseline.json")
        self.baseline = self._load_baseline()
        
    def _load_config(self) -> Dict:
        """
        Load configuration from the config file.
        
        Returns:
            Dict: Configuration settings.
        """
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r') as f:
                    return json.load(f)
            else:
                # Default configuration
                default_config = {
                    "baseline_file": "baseline.json",
                    "hash_algorithm": "sha256",
                    "scan_interval": 3600,  # 1 hour
                    "monitored_paths": [],
                    "excluded_paths": [],
                    "excluded_extensions": [".tmp", ".log", ".bak"],
                    "alert_command": None
                }
                with open(self.config_file, 'w') as f:
                    json.dump(default_config, f, indent=4)
                logger.info(f"Created default configuration file: {self.config_file}")
                return default_config
        except Exception as e:
            logger.error(f"Error loading configuration: {e}")
            sys.exit(1)
    
    def _load_baseline(self) -> Dict:
        """
        Load the baseline hashes from the baseline file.
        
        Returns:
            Dict: Baseline file hashes.
        """
        if os.path.exists(self.baseline_file):
            try:
                with open(self.baseline_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading baseline: {e}")
                return {}
        return {}
    
    def _save_baseline(self) -> None:
        """Save the current baseline to the baseline file."""
        try:
            with open(self.baseline_file, 'w') as f:
                json.dump(self.baseline, f, indent=4)
            logger.info(f"Baseline saved to {self.baseline_file}")
        except Exception as e:
            logger.error(f"Error saving baseline: {e}")
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """
        Calculate the hash of a file.
        
        Args:
            file_path: Path to the file.
            
        Returns:
            str: Hash value of the file.
        """
        hash_algorithm = self.config.get("hash_algorithm", "sha256")
        hash_func = hashlib.new(hash_algorithm)
        
        try:
            with open(file_path, 'rb') as f:
                # Read the file in chunks to handle large files
                for chunk in iter(lambda: f.read(4096), b''):
                    hash_func.update(chunk)
            return hash_func.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating hash for {file_path}: {e}")
            return ""
    
    def _get_file_metadata(self, file_path: str) -> Dict:
        """
        Get file metadata.
        
        Args:
            file_path: Path to the file.
            
        Returns:
            Dict: File metadata.
        """
        try:
            stat_info = os.stat(file_path)
            return {
                "size": stat_info.st_size,
                "permissions": stat_info.st_mode,
                "owner": stat_info.st_uid,
                "group": stat_info.st_gid,
                "last_modified": stat_info.st_mtime,
                "last_accessed": stat_info.st_atime
            }
        except Exception as e:
            logger.error(f"Error getting metadata for {file_path}: {e}")
            return {}
    
    def _should_monitor_file(self, file_path: str) -> bool:
        """
        Check if a file should be monitored based on exclusion rules.
        
        Args:
            file_path: Path to the file.
            
        Returns:
            bool: True if the file should be monitored, False otherwise.
        """
        # Check excluded paths
        for excluded_path in self.config.get("excluded_paths", []):
            if file_path.startswith(excluded_path):
                return False
        
        # Check excluded extensions
        _, extension = os.path.splitext(file_path)
        if extension in self.config.get("excluded_extensions", []):
            return False
            
        return True
    
    def create_baseline(self) -> None:
        """Create a baseline of file hashes for all monitored paths."""
        monitored_paths = self.config.get("monitored_paths", [])
        if not monitored_paths:
            logger.error("No monitored paths specified in configuration")
            return
        
        new_baseline = {}
        file_count = 0
        
        for path in monitored_paths:
            if not os.path.exists(path):
                logger.warning(f"Monitored path does not exist: {path}")
                continue
                
            if os.path.isfile(path):
                if self._should_monitor_file(path):
                    file_hash = self._calculate_file_hash(path)
                    if file_hash:
                        new_baseline[path] = {
                            "hash": file_hash,
                            "metadata": self._get_file_metadata(path),
                            "baseline_date": datetime.now().isoformat()
                        }
                        file_count += 1
            else:
                # Walk through directories
                for root, _, files in os.walk(path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if self._should_monitor_file(file_path):
                            file_hash = self._calculate_file_hash(file_path)
                            if file_hash:
                                new_baseline[file_path] = {
                                    "hash": file_hash,
                                    "metadata": self._get_file_metadata(file_path),
                                    "baseline_date": datetime.now().isoformat()
                                }
                                file_count += 1
        
        self.baseline = new_baseline
        self._save_baseline()
        logger.info(f"Created baseline with {file_count} files")
    
    def check_integrity(self) -> List[Dict]:
        """
        Check the integrity of files against the baseline.
        
        Returns:
            List[Dict]: List of changes detected.
        """
        if not self.baseline:
            logger.error("No baseline available. Create a baseline first.")
            return []
        
        changes = []
        
        # Check existing files against baseline
        for file_path, baseline_data in self.baseline.items():
            if not os.path.exists(file_path):
                changes.append({
                    "file": file_path,
                    "type": "deleted",
                    "timestamp": datetime.now().isoformat()
                })
                continue
                
            current_hash = self._calculate_file_hash(file_path)
            if not current_hash:
                continue
                
            if current_hash != baseline_data["hash"]:
                changes.append({
                    "file": file_path,
                    "type": "modified",
                    "old_hash": baseline_data["hash"],
                    "new_hash": current_hash,
                    "timestamp": datetime.now().isoformat()
                })
        
        # Check for new files
        monitored_paths = self.config.get("monitored_paths", [])
        for path in monitored_paths:
            if not os.path.exists(path):
                continue
                
            if os.path.isfile(path):
                if path not in self.baseline and self._should_monitor_file(path):
                    changes.append({
                        "file": path,
                        "type": "new",
                        "hash": self._calculate_file_hash(path),
                        "timestamp": datetime.now().isoformat()
                    })
            else:
                # Walk through directories
                for root, _, files in os.walk(path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if file_path not in self.baseline and self._should_monitor_file(file_path):
                            changes.append({
                                "file": file_path,
                                "type": "new",
                                "hash": self._calculate_file_hash(file_path),
                                "timestamp": datetime.now().isoformat()
                            })
        
        return changes
    
    def handle_changes(self, changes: List[Dict]) -> None:
        """
        Handle detected changes.
        
        Args:
            changes: List of changes detected.
        """
        if not changes:
            logger.info("No changes detected")
            return
            
        logger.warning(f"Detected {len(changes)} changes:")
        for change in changes:
            if change["type"] == "modified":
                logger.warning(f"MODIFIED: {change['file']}")
                logger.warning(f"  Old hash: {change['old_hash']}")
                logger.warning(f"  New hash: {change['new_hash']}")
            elif change["type"] == "deleted":
                logger.warning(f"DELETED: {change['file']}")
            elif change["type"] == "new":
                logger.warning(f"NEW: {change['file']}")
        
        # Execute alert command if configured
        alert_command = self.config.get("alert_command")
        if alert_command:
            try:
                # Save changes to a temporary file
                temp_file = "fim_changes.json"
                with open(temp_file, 'w') as f:
                    json.dump(changes, f, indent=4)
                
                # Execute alert command
                os.system(f"{alert_command} {temp_file}")
                
                # Clean up
                os.remove(temp_file)
            except Exception as e:
                logger.error(f"Error executing alert command: {e}")
    
    def update_baseline(self, file_path: Optional[str] = None) -> None:
        """
        Update the baseline for a specific file or all files if not specified.
        
        Args:
            file_path: Path to the file to update, or None to update all files.
        """
        if file_path:
            if os.path.exists(file_path) and self._should_monitor_file(file_path):
                file_hash = self._calculate_file_hash(file_path)
                if file_hash:
                    self.baseline[file_path] = {
                        "hash": file_hash,
                        "metadata": self._get_file_metadata(file_path),
                        "baseline_date": datetime.now().isoformat()
                    }
                    self._save_baseline()
                    logger.info(f"Updated baseline for: {file_path}")
            else:
                logger.error(f"Cannot update baseline for {file_path}: File does not exist or is excluded")
        else:
            # Update baseline for all existing files
            self.create_baseline()
    
    def monitor(self) -> None:
        """Start monitoring files based on the configured interval."""
        scan_interval = self.config.get("scan_interval", 3600)  # Default to 1 hour
        
        if not self.baseline:
            logger.warning("No baseline available. Creating a baseline first.")
            self.create_baseline()
        
        logger.info(f"Starting monitoring with scan interval of {scan_interval} seconds")
        
        try:
            while True:
                logger.info("Performing integrity check...")
                changes = self.check_integrity()
                self.handle_changes(changes)
                
                logger.info(f"Next scan in {scan_interval} seconds")
                time.sleep(scan_interval)
        except KeyboardInterrupt:
            logger.info("Monitoring stopped by user")
        except Exception as e:
            logger.error(f"Error during monitoring: {e}")

def main():
    """Main function to run the File Integrity Monitor."""
    parser = argparse.ArgumentParser(description="File Integrity Monitor")
    parser.add_argument("--config", "-c", help="Path to configuration file", default="fim_config.json")
    
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # Create baseline command
    subparsers.add_parser("create-baseline", help="Create a new baseline of file hashes")
    
    # Check integrity command
    subparsers.add_parser("check", help="Check file integrity against the baseline")
    
    # Update baseline command
    update_parser = subparsers.add_parser("update", help="Update the baseline for a specific file or all files")
    update_parser.add_argument("--file", "-f", help="Path to the file to update (optional)")
    
    # Monitor command
    subparsers.add_parser("monitor", help="Start monitoring files continuously")
    
    args = parser.parse_args()
    
    fim = FileIntegrityMonitor(args.config)
    
    if args.command == "create-baseline":
        fim.create_baseline()
    elif args.command == "check":
        changes = fim.check_integrity()
        fim.handle_changes(changes)
    elif args.command == "update":
        fim.update_baseline(args.file)
    elif args.command == "monitor":
        fim.monitor()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()